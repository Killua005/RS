# -*- coding: utf-8 -*-
"""Cleansing&Analysis (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aqRv38zMR7o0wZuw4DjYdZ3B6aFm47-u
"""

# Importing python libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# To make all outputs show
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

file_1= r'C:\Users\ayanb\Downloads\archive\Accident_Information.csv'
file_2= r'C:\Users\ayanb\Downloads\archive\Vehicle_Information.csv'

report_1 = pd.read_csv(file_1,  encoding='unicode_escape',index_col = 0, low_memory = False)
report_2 = pd.read_csv(file_2, encoding = 'unicode_escape', index_col = 0, low_memory = False)

report_1.head(4)
report_2.head(4)

# Inner merge the 2 files into 1 dataset (on Accident_Index column)
dataset = report_1.merge(report_2, on='Accident_Index', how='inner')
dataset.head()

# Getting rid of the accident index, as it is irrelevant to the analysis
dataset.reset_index(inplace = True, drop = True)
dataset.head()
dataset.shape

"""Data contains over 2 million records!"""

#let's take a general look at the variables involved
dataset.info()

#let's take a closer look at nulls within columns
null_columns=dataset.columns[dataset.isnull().any()]
dataset[null_columns].isnull().sum()

#drop variables that are unnecessary
dataset.drop("1st_Road_Number", axis=1, inplace=True)
dataset.drop("2nd_Road_Number", axis=1, inplace=True)
dataset.drop("LSOA_of_Accident_Location", axis=1, inplace=True)
dataset.drop("Location_Easting_OSGR", axis=1, inplace=True)

dataset.drop("Carriageway_Hazards", axis=1, inplace=True)
dataset.drop("Special_Conditions_at_Site", axis=1, inplace=True)
dataset.drop("Hit_Object_in_Carriageway", axis=1, inplace=True)
dataset.drop("Hit_Object_off_Carriageway ", axis=1, inplace=True)

null_columns=dataset.columns[dataset.isnull().any()]
dataset[null_columns].isnull().sum()

dataset.drop("Skidding_and_Overturning", axis=1, inplace=True)
dataset.drop("Hit_Object_off_Carriageway", axis=1, inplace=True)

dataset.drop("2nd_Road_Class", axis=1, inplace=True)
dataset.drop("Driver_IMD_Decile", axis=1, inplace=True)

null_columns=dataset.columns[dataset.isnull().any()]
dataset[null_columns].isnull().sum()

dataset.shape

dataset.dropna(how='any',inplace=True)

dataset.shape

dataset.info()

"""Even though about a quarter or our records were dropped, we are still left with 1.5 million records to analyze. This is still a significant amount of data for the purpose of our analysis. Now that our data is clean of nulls, let's examine it in further detail."""

dataset.head()

"""Let us look further into some of the categorical data to see if they make sense!"""

dataset["1st_Road_Class"].unique()  #unknowns: 'Unclassified'
dataset["Accident_Severity"].unique() #need to reclassify into binary variable for model simplification
#dataset["Date"].unique()
dataset["Day_of_Week"].unique()
dataset["Junction_Control"].unique() #unknowns: 'Data missing or out of range'
dataset["Junction_Detail"].unique() #unknowns: 'Data missing or out of range'
dataset["Light_Conditions"].unique() #unknowns: 'Data missing or out of range'
dataset["Local_Authority_(District)"].unique() #too many categories! might be unnecessary
dataset["Local_Authority_(Highway)"].unique() #too many categories! might be unnecessary
dataset["Police_Force"].unique()
dataset["Road_Surface_Conditions"].unique() #unknowns: 'Data missing or out of range'
dataset["Road_Type"].unique() #unknowns: 'Unknown'
#dataset["Time"].unique()
dataset["Urban_or_Rural_Area"].unique() #unknowns: 'Unallocated' ... is that really unknown?
dataset["Weather_Conditions"].unique() #unknowns: 'Other', 'Unknown','Data missing or out of range'
dataset["InScotland"].unique() #might be an unnecessary location column
dataset["Age_Band_of_Driver"].unique() #unknowns/non-sensical data: 'Data missing or out of range','11 - 15','6 - 10', '0 - 5'
dataset["Driver_Home_Area_Type"].unique() #unknowns: 'Data missing or out of range'
dataset["Journey_Purpose_of_Driver"].unique() #unknowns: 'Other/Not known (2005-10)','Data missing or out of range','Not known','Other'
dataset["Junction_Location"].unique() #unknowns: 'Data missing or out of range'
dataset["make"].unique() #too many categories!
dataset["model"].unique() #too many categories!
dataset["Propulsion_Code"].unique()
dataset["Sex_of_Driver"].unique() #unknowns: 'Not known', 'Data missing or out of range'
dataset["Towing_and_Articulation"].unique() #unknowns: 'Data missing or out of range'
dataset["Vehicle_Leaving_Carriageway"].unique() #unknowns: 'Data missing or out of range'
dataset["Vehicle_Manoeuvre"].unique() #unknowns: 'Data missing or out of range'
dataset["Vehicle_Type"].unique()
dataset["Was_Vehicle_Left_Hand_Drive"].unique() #unknowns: 'Data missing or out of range'
dataset["X1st_Point_of_Impact"].unique() #unknowns: 'Data missing or out of range'

print("1st_Road_Class",":",(dataset["1st_Road_Class"]=='Unclassified').sum())
#print("Carriageway_Hazards",":",(dataset["Carriageway_Hazards"]=='Data missing or out of range').sum())
print("Junction_Control",":",(dataset["Junction_Control"]=='Data missing or out of range').sum())
print("Junction_Detail",":",(dataset["Junction_Detail"]=='Data missing or out of range').sum())
print("Light_Conditions",":",(dataset["Light_Conditions"]=='Data missing or out of range').sum())
print("Road_Surface_Conditions",":",(dataset["Road_Surface_Conditions"]=='Data missing or out of range').sum())
print("Road_Type",":",(dataset["Road_Type"]=='Unknown').sum())
#print("Special_Conditions_at_Site",":",(dataset["Special_Conditions_at_Site"]=='Data missing or out of range').sum())
print("Urban_or_Rural_Area",":",(dataset["Urban_or_Rural_Area"]=='Unallocated').sum())
print("Weather_Conditions",":",(dataset["Weather_Conditions"].isin(['Other','Unknown','Data missing or out of range'])).sum())
print("Age_Band_of_Driver",":",(dataset["Age_Band_of_Driver"].isin(['Data missing or out of range','11 - 15','6 - 10','0 - 5'])).sum())
print("Driver_Home_Area_Type",":",(dataset["Driver_Home_Area_Type"]=='Data missing or out of range').sum())
#print("Hit_Object_in_Carriageway",":",(dataset["Hit_Object_in_Carriageway"]=='Data missing or out of range').sum())
#print("Hit_Object_off_Carriageway",":",(dataset["Hit_Object_off_Carriageway"]=='Data missing or out of range').sum())
print("Journey_Purpose_of_Driver",":",(dataset["Journey_Purpose_of_Driver"].isin(['Other/Not known (2005-10)','Data missing or out of range','Not known','Other'])).sum())
print("Junction_Location",":",(dataset["Junction_Location"]=='Data missing or out of range').sum())
print("Sex_of_Driver",":",(dataset["Sex_of_Driver"].isin(['Not known','Data missing or out of range'])).sum())
#print("Skidding_and_Overturning",":",(dataset["Skidding_and_Overturning"]=='Data missing or out of range').sum())
print("Towing_and_Articulation",":",(dataset["Towing_and_Articulation"]=='Data missing or out of range').sum())
print("Vehicle_Leaving_Carriageway",":",(dataset["Vehicle_Leaving_Carriageway"]=='Data missing or out of range').sum())
print("Vehicle_Manoeuvre",":",(dataset["Vehicle_Manoeuvre"]=='Data missing or out of range').sum())
print("Was_Vehicle_Left_Hand_Drive",":",(dataset["Was_Vehicle_Left_Hand_Drive"]=='Data missing or out of range').sum())
print("X1st_Point_of_Impact",":",(dataset["X1st_Point_of_Impact"]=='Data missing or out of range').sum())

"""As with nulls, we will delete columns that contain mostly meaningless data. Otherwise, we will remove records with remaining meaningless values."""

#drop variables that are unnecessary
dataset.drop("1st_Road_Class", axis=1, inplace=True)
dataset.drop("Junction_Control", axis=1, inplace=True)
dataset.drop("Journey_Purpose_of_Driver", axis=1, inplace=True)

dataset.shape

#otherwise drop records with meaningless data
#dataset=dataset[dataset["Carriageway_Hazards"]!='Data missing or out of range']
dataset=dataset[dataset["Junction_Detail"]!='Data missing or out of range']
dataset=dataset[dataset["Light_Conditions"]!='Data missing or out of range']
dataset=dataset[dataset["Road_Surface_Conditions"]!='Data missing or out of range']
dataset=dataset[dataset["Road_Type"]!='Unknown']
#dataset=dataset[dataset["Special_Conditions_at_Site"]!='Data missing or out of range']
dataset=dataset[dataset["Urban_or_Rural_Area"]!='Unallocated']
dataset=dataset[~dataset["Weather_Conditions"].isin(['Other','Unknown','Data missing or out of range'])]
dataset=dataset[~dataset["Age_Band_of_Driver"].isin(['Data missing or out of range','11 - 15','6 - 10','0 - 5'])]
dataset=dataset[dataset["Driver_Home_Area_Type"]!='Data missing or out of range']
#dataset=dataset[dataset["Hit_Object_in_Carriageway"]!='Data missing or out of range']
#dataset=dataset[dataset["Hit_Object_off_Carriageway"]!='Data missing or out of range']
dataset=dataset[dataset["Junction_Location"]!='Data missing or out of range']
dataset=dataset[~dataset["Sex_of_Driver"].isin(['Not known','Data missing or out of range'])]
#dataset=dataset[dataset["Skidding_and_Overturning"]!='Data missing or out of range']
dataset=dataset[dataset["Towing_and_Articulation"]!='Data missing or out of range']
dataset=dataset[dataset["Vehicle_Leaving_Carriageway"]!='Data missing or out of range']
dataset=dataset[dataset["Vehicle_Manoeuvre"]!='Data missing or out of range']
dataset=dataset[dataset["Was_Vehicle_Left_Hand_Drive"]!='Data missing or out of range']
dataset=dataset[dataset["X1st_Point_of_Impact"]!='Data missing or out of range']

dataset.shape

dataset.info()

#create a new target variable - Reduced target class from a multi-class classification to a binary classification
# problem to handle the imbalanced dataset and simplify analysis


dataset.loc[dataset.Accident_Severity !='Slight', 'Target_Severe_Indicator'] = 1
dataset.loc[dataset.Accident_Severity =='Slight', 'Target_Severe_Indicator'] = 0

dataset["Target_Severe_Indicator"].value_counts()
dataset["Accident_Severity"].value_counts()

"""We see a slight improvement in the unique class counts"""

# Create new column for time of day category

# Define a function that turns the hours into daytime groups

def when_was_it(hour):
    if hour >= 5 and hour < 10:
        return "morning rush (5-10)"
    elif hour >= 10 and hour < 15:
        return "office hours (10-15)"
    elif hour >= 15 and hour < 19:
        return "afternoon rush (15-19)"
    elif hour >= 19 and hour < 23:
        return "evening (19-23)"
    else:
        return "night (23-5)"

# slice first and second string from time column
dataset['Hour'] = dataset['Time'].str[0:2]

# convert new column to numeric datetype
dataset['Hour'] = pd.to_numeric(dataset['Hour'])

# drop null values in our new column
dataset = dataset.dropna(subset=['Hour'])

# cast to integer values
dataset['Hour'] = dataset['Hour'].astype('int')

# apply thus function to our temporary hour column
dataset['Daytime'] = dataset['Hour'].apply(when_was_it)
dataset[['Time', 'Hour', 'Daytime']].head(8)

#drop Did_Police_Officer_Attend_Scene_of_Accident and Police_Force, as they are irrelevant. The dataset to begin
# with represents only those accidents reported by the police
dataset.drop("Did_Police_Officer_Attend_Scene_of_Accident", axis=1, inplace=True)
dataset.drop("Police_Force", axis=1, inplace=True)

#drop Time as Daytime is sufficient
dataset.drop("Time", axis=1, inplace=True)


#drop Year_y as there is already a column called year_x containing the same info
dataset.drop("Year_y", axis=1, inplace=True)

#since 'model' is a more detailed version of 'make', drop 'model' and keep 'make'
dataset.drop("model", axis=1, inplace=True)

#we can drop inscotland, as the data contained in this column is higly skewed and will not proivde much info
dataset.drop("InScotland", axis=1, inplace=True)

#we can drop 'Vehicle_reference', due to high correlation with 'Number_of_Vehicles'
dataset.drop("Vehicle_Reference", axis=1, inplace=True)

#we can drop 'Local_Authority_(District)' since it is a more detailed version of 'Local_Authority_(Highway)'
dataset.drop("Local_Authority_(District)", axis=1, inplace=True)

dataset.shape

#check correlation between 'Was_Vehicle_Left_Hand_Drive' and  'Target_Severe_Indicator'

dataset.loc[dataset['Was_Vehicle_Left_Hand_Drive'] == 'Yes', 'Was_Vehicle_Left_Hand_Drive_test'] = 1
dataset.loc[dataset['Was_Vehicle_Left_Hand_Drive'] == 'No', 'Was_Vehicle_Left_Hand_Drive_test'] = 0

x=dataset['Target_Severe_Indicator'].values
y=dataset['Was_Vehicle_Left_Hand_Drive_test'].values

np.corrcoef(x,y)

# Due to the low correlation with the target variable and high imbalance in values, we drop 'Was_Vehicle_Left_Hand_Drive'
dataset.drop("Was_Vehicle_Left_Hand_Drive", axis=1, inplace=True)

dataset.shape

dataset.info()

# change numeric values in pedestrian crossing variables to categoric
# Converting numerical values present in these columns to a more meaninful form using data obtained from the data_dictionary

dataset.loc[dataset['Pedestrian_Crossing-Human_Control'] ==0, 'Ped_Cross_Human'] = 'None within 50 metres'
dataset.loc[dataset['Pedestrian_Crossing-Human_Control'] ==1, 'Ped_Cross_Human'] = 'Control by school crossing patrol'
dataset.loc[dataset['Pedestrian_Crossing-Human_Control'] ==2, 'Ped_Cross_Human'] = 'Control by other authorised person'


dataset.loc[dataset['Pedestrian_Crossing-Physical_Facilities'] ==0, 'Ped_Cross_Physical'] = 'No physical crossing facilities within 50 metres'
dataset.loc[dataset['Pedestrian_Crossing-Physical_Facilities'] ==1, 'Ped_Cross_Physical'] = 'Zebra'
dataset.loc[dataset['Pedestrian_Crossing-Physical_Facilities'] ==4, 'Ped_Cross_Physical'] = 'Pelican, puffin, toucan or similar non-junction pedestrian light crossing'
dataset.loc[dataset['Pedestrian_Crossing-Physical_Facilities'] ==5, 'Ped_Cross_Physical'] = 'Pedestrian phase at traffic signal junction'
dataset.loc[dataset['Pedestrian_Crossing-Physical_Facilities'] ==7, 'Ped_Cross_Physical'] = 'Footbridge or subway'
dataset.loc[dataset['Pedestrian_Crossing-Physical_Facilities'] ==8, 'Ped_Cross_Physical'] = 'Central refuge'

#and then we can drop the original pedestrian crossing variables
dataset.drop("Pedestrian_Crossing-Human_Control", axis=1, inplace=True)
dataset.drop("Pedestrian_Crossing-Physical_Facilities", axis=1, inplace=True)

dataset.info()

#Change categorical variables with order to numeric
dataset['Age_Band_of_Driver'].unique()

dataset.loc[dataset['Age_Band_of_Driver'] =='16 - 20', 'Age_Band_of_Driver_order'] = 1
dataset.loc[dataset['Age_Band_of_Driver'] =='21 - 25', 'Age_Band_of_Driver_order'] = 2
dataset.loc[dataset['Age_Band_of_Driver'] =='26 - 35', 'Age_Band_of_Driver_order'] = 3
dataset.loc[dataset['Age_Band_of_Driver'] =='36 - 45', 'Age_Band_of_Driver_order'] = 4
dataset.loc[dataset['Age_Band_of_Driver'] =='46 - 55', 'Age_Band_of_Driver_order'] = 5
dataset.loc[dataset['Age_Band_of_Driver'] =='56 - 65', 'Age_Band_of_Driver_order'] = 6
dataset.loc[dataset['Age_Band_of_Driver'] =='66 - 75', 'Age_Band_of_Driver_order'] = 7
dataset.loc[dataset['Age_Band_of_Driver'] =='Over 75', 'Age_Band_of_Driver_order'] = 8

dataset.drop("Age_Band_of_Driver", axis=1, inplace=True)

dataset.shape

eda_data_1=dataset.copy()

"""Next we carry out some descriptive analysis, to gain insights about our data

# EDA

## Time Series Analysis

###Total Number of Accidents per Year
"""

# Covert 'Date' to proper datetime format
eda_data_1['Date']= pd.to_datetime(eda_data_1['Date'], format="%Y-%m-%d")

eda_data_1.info()

eda_data_1.iloc[:, 1:5].info()

# In general terms, the number of accidents have gone up over the years
eda_data_1["Year_x"].value_counts().sort_index()

# Plotting Number of Accidents over the years

# Getting yearly count
yearly_count = eda_data_1['Date'].dt.year.value_counts().sort_index(ascending=False)

# prepare plot
sns.set_style('white')
fig, ax = plt.subplots(figsize=(12,5))

# plot
ax.bar(yearly_count.index, yearly_count.values, color='skyblue')
ax.plot(yearly_count, linestyle=':', color='black')
ax.set_title('\nTotal Number of Accidents per Year\n', fontsize=14, fontweight='bold')
ax.set(ylabel='\nNumber of Accidents')
ax.set(xlabel='\nYears')
# remove all spines
sns.despine(ax=ax, top=True, right=True, left=True, bottom=True);

"""## Average Number of Accidents by Weekday"""

# Total number of accidents by day of the week
eda_data_1["Day_of_Week"].value_counts()

eda_data_1.info()

import pandas as pd

# Count accidents for each day from 2005 to 2016
weekday_counts = eda_data_1.set_index('Date').resample('D').size().reset_index(name='Count')

# Extracting weekday names
weekday_counts['Weekday'] = weekday_counts['Date'].dt.day_name()

# Calculating average accidents per weekday
weekday_averages = weekday_counts.groupby('Weekday')['Count'].mean().reset_index()
weekday_averages.columns = ['Weekday', 'Average_Accidents']

# Setting 'Weekday' as index for better visualization
weekday_averages.set_index('Weekday', inplace=True)

# Displaying the result
weekday_averages

import seaborn as sns
import matplotlib.pyplot as plt

# Reorder the weekdays starting with Monday (backwards due to printing behavior)
days = ['Sunday', 'Saturday', 'Friday', 'Thursday', 'Wednesday', 'Tuesday', 'Monday']

# Prepare the plot
sns.set_style('white')
fig, ax = plt.subplots(figsize=(10, 5))

# Define colors for the bars
colors = ['lightsteelblue'] * len(days)  # All bars will be lightsteelblue

# Plot the data
weekday_averages.reindex(days).plot(kind='barh', ax=ax, color=colors)

# Set the title and labels
ax.set_title('\nAverage Number of Accidents by Weekday\n', fontsize=14, fontweight='bold')
ax.set(xlabel='\nAverage Number of Accidents', ylabel='')

# Remove the legend (if you don't have multiple datasets to display)
ax.legend([])

# Remove all spines for a cleaner look
sns.despine(ax=ax, top=True, right=True, left=True, bottom=True)

# Show the plot
plt.show()

"""Number of Accidents by Day and Year"""

# Getting weekday and year
weekday = eda_data_1['Date'].dt.day_name()  # Use day_name() instead of weekday_name
year = eda_data_1['Date'].dt.year

# Grouping data by year and weekday
accident_table = eda_data_1.groupby([year, weekday]).size()

# Restructuring the data
accident_table = accident_table.rename_axis(['Year', 'Weekday'])\
                                 .unstack('Weekday')\
                                 .reindex(columns=days)  # Ensure 'days' is defined as your desired order of weekdays

accident_table

# Plotting dataframe
plt.figure(figsize=(10,6))
sns.heatmap(accident_table, cmap='Reds')
plt.title('\nAccidents by Year and Weekday\n', fontsize=14, fontweight='bold')
plt.xlabel('')
plt.ylabel('');

import pandas as pd

# Create a subset of the DataFrame
sub_df = eda_data_1[['Date', 'Accident_Severity']]

# Group by week and severity
count_of_fatalities = sub_df.set_index('Date').groupby([pd.Grouper(freq='W'), 'Accident_Severity']).size()

# Build a nice table
fatalities_table = (
    count_of_fatalities
    .rename_axis(['Week', 'Accident_Severity'])
    .unstack('Accident_Severity', fill_value=0)  # Handle missing values by filling with 0
    .rename(columns={1: 'fatal', 2: 'serious', 3: 'slight'})
)

# Display the first few rows of the table
print(fatalities_table.head())

fatalities_table['sum'] = fatalities_table.sum(axis=1)
fatalities_table = fatalities_table.join(fatalities_table.div(fatalities_table['sum'], axis=0), rsuffix='_percentage')
fatalities_table.head()

# prepare data
sub_df = fatalities_table[['Fatal_percentage', 'Serious_percentage', 'Slight_percentage']]

# prepare plot
sns.set_style('white')
fig, ax = plt.subplots(figsize=(14,6))
colors=['black', 'navy', 'lightsteelblue']

# plot
sub_df.plot(color=colors, ax=ax)
ax.set_title('\nProportion of Accident Severities\n', fontsize=14, fontweight='bold')
ax.set(ylabel='Share of All Accidents\n', xlabel='Year')
ax.legend(labels=['Fatal Accidents', 'Serious Accidents', 'Slight Accidents'],
          bbox_to_anchor=(1.3, 1.1), frameon=False)

# remove all spines
sns.despine(top=True, right=True, left=True, bottom=False);

"""Accident Distribution per Day"""

# prepare plot
sns.set_style('white')
fig, ax = plt.subplots(figsize=(10,6))

# plot
eda_data_1.Hour.hist(bins=24, ax=ax, color='lightsteelblue')
ax.set_title('\nAccidents by Time of Day\n', fontsize=14, fontweight='bold')
ax.set(xlabel='Hour of the Day', ylabel='Number of Accidents')

# remove all spines
sns.despine(top=True, right=True, left=True, bottom=True);

"""Count of accidents by 'Daytime' categories"""

# prepare dataframe
order = ['night (23-5)', 'evening (19-23)', 'afternoon rush (15-19)', 'office hours (10-15)', 'morning rush (5-10)']
df_sub = eda_data_1.groupby('Daytime').size().reindex(order)

# prepare barplot
fig, ax = plt.subplots(figsize=(10, 5))
colors = ['lightsteelblue', 'lightsteelblue', 'navy', 'lightsteelblue', 'lightsteelblue']

# plot
df_sub.plot(kind='barh', ax=ax, color=colors)
ax.set_title('\nAccidents by Daytime Categories\n', fontsize=14, fontweight='bold')
ax.set(xlabel='\nNumber of Accidents', ylabel='')

# remove all spines
sns.despine(top=True, right=True, left=True, bottom=True);

"""Percentage Distributiong of Accident Severity"""

# assign the data
fatal   = eda_data_1.Accident_Severity.value_counts()['Fatal']
serious = eda_data_1.Accident_Severity.value_counts()['Serious']
slight  = eda_data_1.Accident_Severity.value_counts()['Slight']

names = ['Fatal Accidents','Serious Accidents', 'Slight Accidents']
size  = [fatal, serious, slight]
#explode = (0.2, 0, 0)

# create a pie chart
plt.pie(x=size, labels=names, colors=['red', 'darkorange', 'silver'],
        autopct='%1.2f%%', pctdistance=0.6, textprops=dict(fontweight='bold'),
        wedgeprops={'linewidth':7, 'edgecolor':'white'})

# create circle for the center of the plot to make the pie look like a donut
my_circle = plt.Circle((0,0), 0.6, color='white')

# plot the donut chart
fig = plt.gcf()
fig.set_size_inches(8,8)
fig.gca().add_artist(my_circle)
plt.title('\nAccident Severity: Proportion in % (2005-2016)', fontsize=14, fontweight='bold')
plt.show();

"""Share of accident sevirity by Daytime"""

# prepare dataframe with simple counts
counts = eda_data_1.groupby(['Daytime', 'Accident_Severity']).size()

counts = counts.rename_axis(['Daytime', 'Accident_Severity'])\
                                .unstack('Accident_Severity')\
                                .rename({1:'fatal', 2:'serious', 3:'slight'}, axis='columns')
counts

# prepare dataframe with shares
counts['sum'] = counts.sum(axis=1)
counts = counts.join(counts.div(counts['sum'], axis=0), rsuffix=' in %')
counts_share = counts.drop(columns=['Fatal', 'Serious', 'Slight', 'sum', 'sum in %'], axis=1)
counts_share

# prepare barplot
fig, ax = plt.subplots(figsize=(10, 5))

# plot
counts_share.reindex(order).plot(kind='barh', ax=ax, stacked=True, cmap='cividis')
ax.set_title('\nAccident Severity by Daytime Categories\n', fontsize=14, fontweight='bold')
ax.set(xlabel='Percent Accident Severity', ylabel='')
ax.legend(bbox_to_anchor=(1.25, 0.98), frameon=False)

# remove all spines
sns.despine(top=True, right=True, left=True, bottom=True);

"""Reason for Accidents"""

vehicle_ManoeuvreDF=eda_data_1["Vehicle_Manoeuvre"].value_counts()
vehicle_ManoeuvreDF

print(type(vehicle_ManoeuvreDF))

from pandas import Series, DataFrame
df=Series.to_frame(vehicle_ManoeuvreDF)

df.columns = ['Count']

df.reset_index(level=0, inplace=True)

list(df.columns.values)

df

df.columns = ['Vehicle_Manoeuvre', 'Count']

manoeuvre = df['Vehicle_Manoeuvre'].tolist()
count_manoeuvre = df['Count'].tolist()

import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected=True)

pie_visual = go.Pie(labels=manoeuvre, values=count_manoeuvre, marker=dict(colors=['#25e475', '#ee1c96',]))

layout = go.Layout(title='Vehicle manoeuvre at the time of accident', width=800, height=500)
fig = go.Figure(data=[pie_visual], layout=layout)
iplot(fig)

"""Accidents categorized by gender & age_band of driver"""

eda_data_1['Sex_of_Driver'].value_counts()

df_gender_male =  eda_data_1['Sex_of_Driver']=='Male'
df_male=eda_data_1[df_gender_male]
df_male.head(3)

# Filter for female drivers
df_female = eda_data_1[eda_data_1['Sex_of_Driver'] == 'Female']

# Count occurrences of each age band for male and female drivers
male_counts = eda_data_1[eda_data_1['Sex_of_Driver'] == 'Male']['Age_Band_of_Driver_order'].value_counts()
female_counts = df_female['Age_Band_of_Driver_order'].value_counts()

# Convert the Series to DataFrames
maleDF = male_counts.reset_index()
femaleDF = female_counts.reset_index()

# Rename columns
maleDF.columns = ['Age_Band_of_Driver', 'Count']
femaleDF.columns = ['Age_Band_of_Driver', 'Count']

# Display the top 2 entries for both DataFrames
print("Male Age Band Counts:")
print(maleDF.head(2))
print("\nFemale Age Band Counts:")
print(femaleDF.head(2))

"""Accidents by gender and age band.png"""

trace1 = go.Bar(
    x=male_age_band,
    y=male_accident_count,
    name='Male Drivers',
    marker=dict(color='#5e59f2'),
    opacity=0.8
)
trace2 = go.Bar(
    x=female_age_band,
    y=female_accident_count,
    name='Female Drivers',
    marker=dict(color='#F259D6'),
    opacity=0.8
)
data = [trace1, trace2]
layout = go.Layout(
    barmode='group',
    legend=dict(dict(x=-.1, y=1.2)),
    margin=dict(b=120),
    title = 'Accidents categorized by Gender and Age Group',
)

fig = go.Figure(data=data, layout=layout)
iplot(fig, filename='grouped-bar')

plt.show()

"""Average number of casualities by Daytime"""

# Create a dictionary of Daytime groups
daytime_groups = {1: 'Morning (5-10)',
                  2: 'Office Hours (10-15)',
                  3: 'Afternoon Rush (15-19)',
                  4: 'Evening (19-23)',
                  5: 'Night(23-5)'}

labels = tuple(daytime_groups.values())

# plot average no. of casualties by daytime
eda_data_1.groupby('Daytime')['Number_of_Casualties'].mean().plot(kind='bar', color='slategrey',
                                                                 figsize=(12,4), grid=False)
plt.xticks(np.arange(5), labels, rotation='horizontal')
plt.ylim((1,1.5))
plt.xlabel(''), plt.ylabel('Average Number of Casualties\n')
plt.title('\nAverage Number of Casualties by Daytime\n', fontweight='bold')
sns.despine(top=True, right=True, left=True, bottom=True);

"""Average number of casualities by speed limit"""

eda_data_1.groupby('Speed_limit')['Number_of_Casualties'].mean()

# check speed limit
eda_data_1.groupby('Speed_limit')['Number_of_Casualties'].mean().plot(kind='bar', color='slategrey',
                                                              figsize=(15,4), grid=False)
plt.xticks(np.arange(8),
           ['10mph', '15mph', '20mph', '30mph', '40mph', '50mph', '60mph', '70mph'],
           rotation='horizontal')
plt.ylim((0.6,2.0))
plt.xlabel(''), plt.ylabel('Average Number of Casualties\n')
plt.title('\nAverage Number of Casualties by Speed Limit\n', fontweight='bold')
sns.despine(top=True, right=True, left=True, bottom=True);

